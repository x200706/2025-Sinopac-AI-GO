{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12097625,"sourceType":"datasetVersion","datasetId":7615931},{"sourceId":428757,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":349494,"modelId":370755}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"MEMO：Kaggle代碼為最終版本","metadata":{}},{"cell_type":"code","source":"\"\"\"\n在地端執行時請註解此段程式\n\"\"\"\n!pip install scikit-learn==1.2.2\n!pip install imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:02.149687Z","iopub.execute_input":"2025-06-08T15:34:02.149981Z","iopub.status.idle":"2025-06-08T15:34:08.285412Z","shell.execute_reply.started":"2025-06-08T15:34:02.149962Z","shell.execute_reply":"2025-06-08T15:34:08.284605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 導入庫\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport dask\nimport joblib\nimport gc\nimport os\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score, average_precision_score, confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\nfrom sklearn.metrics import make_scorer, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_cell_guid":"79ba8737-9354-4526-8388-c9a3167fb858","_uuid":"68ce0588-31f4-42ac-bd8a-fbebcee925d7","collapsed":false,"execution":{"iopub.status.busy":"2025-06-08T15:34:08.287014Z","iopub.execute_input":"2025-06-08T15:34:08.287267Z","iopub.status.idle":"2025-06-08T15:34:08.293230Z","shell.execute_reply.started":"2025-06-08T15:34:08.287244Z","shell.execute_reply":"2025-06-08T15:34:08.292451Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 參數設置\nRUN_ON_KAGGLE = True\nEXPORT_DATASET_FOR_KAGGEL = False\n\n# 正:負比例實驗\n# TARGET_RATIO = 60 # 1:60，即不使用VAE時 F1為0.8333\nTARGET_RATIO = 50 # 1:50時 F1為0.8379 👑快又有效的比例\n# TARGET_RATIO = 25 # 1:25時 F1為0.8379\n# TARGET_RATIO = 1 # 1:1時 F1為0.8379\n\nINIT_MODEL = False\n\n# for VAE\nLATENT_DIM = 12000 # VAE 潛在空間維度；12000是原本的特徵欄位大略數目，後續會刷新為 上市加權指數XXX 的欄位數目\nVAE_EPOCHS = 150 # VAE 訓練輪數\nVAE_BATCH_SIZE = 16 # VAE 批次大小\nRE_TRAIN_VAE = False # VAE_EPOCHS跟VAE_BATCH_SIZE有改變時，要改變這個設定以重新訓練新的VAE模型","metadata":{"_cell_guid":"f26ec7cd-f75f-4575-a538-d891726de703","_uuid":"f6278279-18ee-4588-9ae0-8b76b51e5995","collapsed":false,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.495449Z","iopub.execute_input":"2025-06-08T15:34:38.496007Z","iopub.status.idle":"2025-06-08T15:34:38.500024Z","shell.execute_reply.started":"2025-06-08T15:34:38.495983Z","shell.execute_reply":"2025-06-08T15:34:38.499214Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n自家電腦僅作資料前處理，不做VAE模型訓練，故不import tf相關套件\n\"\"\"\nif RUN_ON_KAGGLE:\n    # for VAE\n    # pip install tensorflow\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda, Layer, Reshape, Flatten, ReLU, LeakyReLU\n    from tensorflow.keras.layers import BatchNormalization\n    from tensorflow.keras.models import Model, Sequential\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.losses import mse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.502402Z","iopub.execute_input":"2025-06-08T15:34:38.502611Z","iopub.status.idle":"2025-06-08T15:34:38.517084Z","shell.execute_reply.started":"2025-06-08T15:34:38.502588Z","shell.execute_reply":"2025-06-08T15:34:38.516391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not RUN_ON_KAGGLE:\n    \"\"\"\n    使用缺失值填補+欠採樣完的資料集進行預處理\n    \"\"\"\n    # 讀取 CSV\n    print('載入訓練集以進行預處理...')\n    X_train_ddf = dd.read_csv('./X_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n    y_train_ddf = dd.read_csv('./y_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    print('載入測試集以進行預處理...')\n    X_test_ddf = dd.read_csv('./X_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n    y_test_ddf = dd.read_csv('./y_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.518135Z","iopub.execute_input":"2025-06-08T15:34:38.518385Z","iopub.status.idle":"2025-06-08T15:34:38.530474Z","shell.execute_reply.started":"2025-06-08T15:34:38.518363Z","shell.execute_reply":"2025-06-08T15:34:38.529911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not RUN_ON_KAGGLE:\n    print('精煉特徵...')\n    feature_importance_df = pd.read_csv('feature_importance.csv')\n    # 改用上市加權指數XXX，節省效能+保底F1 0.81\n    mask = feature_importance_df['Feature'].str.startswith('上市加權指數', na=False)\n    important_features = feature_importance_df[mask]['Feature'].tolist()\n    print(f\"保留的特徵數量 (基於前綴 '上市加權指數'): {len(important_features)}\")\n    print(f\"保留的特徵 (基於前綴 '上市加權指數'):\", important_features)\n\n    # 刷新VAE潛在空間維度參數\n    LATENT_DIM = len(important_features)\n\n    X_train_ddf = X_train_ddf[important_features]\n    X_test_ddf = X_test_ddf[important_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.531519Z","iopub.execute_input":"2025-06-08T15:34:38.531787Z","iopub.status.idle":"2025-06-08T15:34:38.545297Z","shell.execute_reply.started":"2025-06-08T15:34:38.531768Z","shell.execute_reply":"2025-06-08T15:34:38.544516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if (not RUN_ON_KAGGLE) and EXPORT_DATASET_FOR_KAGGEL :\n    # 匯出到CSV\n    print(\"匯出CSV以清空計算圖\")\n    X_test_ddf.to_csv('./X_test_ddf_kaggle.csv', index=False)\n    # y_train_ddf.to_csv('./y_train_ddf_kaggle.csv', index=False) # Y本來就只有一欄，不用花時間重新匯出\n    X_train_ddf.to_csv('./X_train_ddf_kaggle.csv', index=False)\n    # y_test_ddf.to_csv('./y_test_ddf_kaggle.csv', index=False) # Y本來就只有一欄，不用花時間重新匯出\n    print(\"所有資料已匯出到 CSV！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.546481Z","iopub.execute_input":"2025-06-08T15:34:38.546747Z","iopub.status.idle":"2025-06-08T15:34:38.562891Z","shell.execute_reply.started":"2025-06-08T15:34:38.546727Z","shell.execute_reply":"2025-06-08T15:34:38.562213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n自家電腦僅作資料前處理，不做VAE模型訓練，故不需要Pandas df\n\"\"\"\n# if not RUN_ON_KAGGLE:\n#     \"\"\"\n#     此時才正式將ddf正式轉成Pandas df\n#     \"\"\"\n#     print(\"轉換訓練集格式為熊貓DF...\")\n#     # 初始化空的 DataFrame\n#     X_train = pd.DataFrame()\n#     y_train = pd.DataFrame()\n\n#     # 分批合併\n#     batch_size = 10  # 每次處理 10 個分割\n#     for i in range(0, X_train_ddf.npartitions, batch_size):\n#         print(f\"處理批次 {i//batch_size + 1}\")\n#         # 一次計算多個分割\n#         X_batch = X_train_ddf.partitions[i:min(i + batch_size, X_train_ddf.npartitions)].compute()\n#         y_batch = y_train_ddf.partitions[i:min(i + batch_size, y_train_ddf.npartitions)].compute()\n#         # 合併到結果\n#         X_train = pd.concat([X_train, X_batch], axis=0, ignore_index=True)\n#         y_train = pd.concat([y_train, y_batch], axis=0, ignore_index=True)\n#         del X_batch, y_batch\n#         gc.collect()\n\n#     print(\"轉換測試集格式為熊貓DF...\")\n#     # 初始化空的 DataFrame\n#     X_test = pd.DataFrame()\n#     y_test = pd.DataFrame()\n\n#     # 分批合併\n#     batch_size = 10  # 每次處理 10 個分割\n#     for i in range(0, X_test_ddf.npartitions, batch_size):\n#         print(f\"處理批次 {i//batch_size + 1}\")\n#         # 一次計算多個分割\n#         X_batch = X_test_ddf.partitions[i:min(i + batch_size, X_test_ddf.npartitions)].compute()\n#         y_batch = y_test_ddf.partitions[i:min(i + batch_size, y_test_ddf.npartitions)].compute()\n#         # 合併到結果\n#         X_test = pd.concat([X_test, X_batch], axis=0, ignore_index=True)\n#         y_test = pd.concat([y_test, y_batch], axis=0, ignore_index=True)\n#         del X_batch, y_batch\n#         gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.563517Z","iopub.execute_input":"2025-06-08T15:34:38.563729Z","iopub.status.idle":"2025-06-08T15:34:38.577052Z","shell.execute_reply.started":"2025-06-08T15:34:38.563692Z","shell.execute_reply":"2025-06-08T15:34:38.576519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n自家電腦僅作資料前處理，不做VAE模型訓練，提前結束程式\n\"\"\"\nimport sys\nif not RUN_ON_KAGGLE:\n    sys.exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.578387Z","iopub.execute_input":"2025-06-08T15:34:38.578564Z","iopub.status.idle":"2025-06-08T15:34:38.593543Z","shell.execute_reply.started":"2025-06-08T15:34:38.578551Z","shell.execute_reply":"2025-06-08T15:34:38.592975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_ON_KAGGLE:\n    \"\"\"\n    使用缺失值填補+欠採樣完+僅留上市加權指數XXX的資料集\n    \"\"\"\n    # 讀取 CSV\n    print(\"轉換訓練集格式為熊貓DF...\")\n    X_train_ddf = dd.read_csv('/kaggle/input/taiwan-stock/X_train_ddf_kaggle.csv/*', sample=1048576, blocksize='128MB') #TODO 應該改成Kaggle實際目錄\n    y_train_ddf = dd.read_csv('/kaggle/input/taiwan-stock/y_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    # 初始化空的 DataFrame\n    X_train = pd.DataFrame()\n    y_train = pd.DataFrame()\n\n    # 分批合併\n    batch_size = 10  # 每次處理 10 個分割\n    for i in range(0, X_train_ddf.npartitions, batch_size):\n        print(f\"處理批次 {i//batch_size + 1}\")\n        # 一次計算多個分割\n        X_batch = X_train_ddf.partitions[i:min(i + batch_size, X_train_ddf.npartitions)].compute()\n        y_batch = y_train_ddf.partitions[i:min(i + batch_size, y_train_ddf.npartitions)].compute()\n        # 合併到結果\n        X_train = pd.concat([X_train, X_batch], axis=0, ignore_index=True)\n        y_train = pd.concat([y_train, y_batch], axis=0, ignore_index=True)\n        del X_batch, y_batch\n        gc.collect()\n\n    # 讀取 CSV\n    print(\"轉換測試集格式為熊貓DF...\")\n    X_test_ddf = dd.read_csv('/kaggle/input/taiwan-stock/X_test_ddf_kaggle.csv/*', sample=1048576, blocksize='128MB')\n    y_test_ddf = dd.read_csv('/kaggle/input/taiwan-stock/y_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    # 初始化空的 DataFrame\n    X_test = pd.DataFrame()\n    y_test = pd.DataFrame()\n\n    # 分批合併\n    batch_size = 10  # 每次處理 10 個分割\n    for i in range(0, X_test_ddf.npartitions, batch_size):\n        print(f\"處理批次 {i//batch_size + 1}\")\n        # 一次計算多個分割\n        X_batch = X_test_ddf.partitions[i:min(i + batch_size, X_test_ddf.npartitions)].compute()\n        y_batch = y_test_ddf.partitions[i:min(i + batch_size, y_test_ddf.npartitions)].compute()\n        # 合併到結果\n        X_test = pd.concat([X_test, X_batch], axis=0, ignore_index=True)\n        y_test = pd.concat([y_test, y_batch], axis=0, ignore_index=True)\n        del X_batch, y_batch\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.614659Z","iopub.execute_input":"2025-06-08T15:34:38.615236Z","iopub.status.idle":"2025-06-08T15:34:46.629000Z","shell.execute_reply.started":"2025-06-08T15:34:38.615220Z","shell.execute_reply":"2025-06-08T15:34:46.628168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 計算正負樣本權重\nneg_count_result = (y_train == 0).sum() # 要留意Pandas df跟Dask dd計算行為有些許不同\npos_count_result = (y_train == 1).sum()\n\n# 打印類型以調試\nprint(f\"Type of neg_count_result: {type(neg_count_result)}\")\nprint(f\"Value of neg_count_result: {neg_count_result}\")\nprint(f\"Type of pos_count_result: {type(pos_count_result)}\")\nprint(f\"Value of pos_count_result: {pos_count_result}\")\n\n# 從可能的 Series 中提取純量\nneg_count = neg_count_result.item() if isinstance(neg_count_result, pd.Series) else neg_count_result\npos_count = pos_count_result.item() if isinstance(pos_count_result, pd.Series) else pos_count_result\n\nscale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n\nprint(f\"Negative count: {neg_count}\")\nprint(f\"Positive count: {pos_count}\")\nprint(f\"Scale positive weight: {scale_pos_weight}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.630300Z","iopub.execute_input":"2025-06-08T15:34:46.630533Z","iopub.status.idle":"2025-06-08T15:34:46.638422Z","shell.execute_reply.started":"2025-06-08T15:34:46.630516Z","shell.execute_reply":"2025-06-08T15:34:46.637851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n此塊for匯入.keras模型\n\"\"\"\n# KL Divergence Loss Layer\nclass KLDivergenceLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(KLDivergenceLayer, self).__init__(**kwargs)\n\n    def call(self, inputs_for_kl): # inputs_for_kl 應該是 [z_mean, z_log_var]\n        z_mean_kl, z_log_var_kl = inputs_for_kl\n        kl_batch = 1 + z_log_var_kl - tf.square(z_mean_kl) - tf.exp(z_log_var_kl)\n        kl_batch = tf.reduce_sum(kl_batch, axis=-1) * -0.5\n        self.add_loss(tf.reduce_mean(kl_batch)) # 添加 KL 損失的均值到層的損失\n        return inputs_for_kl # 返回原始輸入，不改變數據流\n\n# Sampling Layer (輸入仍然是原始的 z_mean 和 z_log_var)\ndef sampling(args):\n    z_mean_s, z_log_var_s = args\n    batch = tf.shape(z_mean_s)[0]\n    dim = tf.shape(z_mean_s)[1]\n    epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=1.0)\n    return z_mean_s + tf.exp(0.5 * z_log_var_s) * epsilon\n\n# 損失函數現在只需要計算重構損失\ndef reconstruction_loss_fn(y_true, y_pred):\n    return tf.reduce_mean(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.639299Z","iopub.execute_input":"2025-06-08T15:34:46.639557Z","iopub.status.idle":"2025-06-08T15:34:46.652852Z","shell.execute_reply.started":"2025-06-08T15:34:46.639541Z","shell.execute_reply":"2025-06-08T15:34:46.652216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nVAE過採樣至指定比例\n\"\"\"\nprint(\"\\n================ VAE 模型訓練 ================\")\n#TODO 要改成Kaggle路徑 注意要上傳到input\nvae_model_path = 'my_vae.keras'\ngenerator_model_path = 'my_vae_generator.keras'\nX_train_minority = X_train[y_train.values == 1] # 這裡決定生成目標\n\nif os.path.exists('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae.keras') and os.path.exists('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae_generator.keras') and RE_TRAIN_VAE != True: # 如果目錄下已經存在VAE模型，代表已訓練完成，無須重新訓練\n    print(\"\\n正在載入模型...\")\n    custom_objects_dict = {\n        'sampling': sampling,\n        'reconstruction_loss_fn': reconstruction_loss_fn\n    }\n    vae = tf.keras.models.load_model('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae.keras', custom_objects=custom_objects_dict)\n    print(\"VAE 模型已載入。\")\n    vae.summary()\n\n    generator = tf.keras.models.load_model('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae_generator.keras')\n    print(\"Generator 模型已載入。\")\n    generator.summary()\nelse:\n    print(f\"用於訓練 VAE 的少數類別樣本數量: {len(X_train_minority)}\")\n    original_dim = X_train_minority.shape[1]\n\n    # VAE Encoder\n    inputs_vae = Input(shape=(original_dim,), name='vae_input')\n    h_vae = Dense(int(original_dim * 2.0), activation='relu')(inputs_vae)\n    h_vae = BatchNormalization()(h_vae)\n    h_vae = Dense(int(original_dim * 1.25), activation='relu')(h_vae)\n    z_mean_tensor = Dense(LATENT_DIM, name='z_mean_encoder_output')(h_vae)\n    z_log_var_tensor = Dense(LATENT_DIM, name='z_log_var_encoder_output')(h_vae)\n\n    # KL Divergence Loss Layer\n    class KLDivergenceLayer(tf.keras.layers.Layer):\n        def __init__(self, **kwargs):\n            super(KLDivergenceLayer, self).__init__(**kwargs)\n\n        def call(self, inputs_for_kl): # inputs_for_kl 應該是 [z_mean, z_log_var]\n            z_mean_kl, z_log_var_kl = inputs_for_kl\n            kl_batch = 1 + z_log_var_kl - tf.square(z_mean_kl) - tf.exp(z_log_var_kl)\n            kl_batch = tf.reduce_sum(kl_batch, axis=-1) * -0.5\n            self.add_loss(tf.reduce_mean(kl_batch)) # 添加 KL 損失的均值到層的損失\n            return inputs_for_kl # 返回原始輸入，不改變數據流\n\n    # 將 KL 散度計算層應用於 encoder 的輸出\n    # 這個層的 call 方法會執行 self.add_loss()\n    # _ (下劃線) 表示我們不直接使用這個層的輸出進行後續計算（因為它返回輸入）\n    # 但重要的是它被執行了。\n    _ = KLDivergenceLayer(name='kl_divergence_adder')([z_mean_tensor, z_log_var_tensor])\n\n\n    # Sampling Layer (輸入仍然是原始的 z_mean 和 z_log_var)\n    def sampling(args):\n        z_mean_s, z_log_var_s = args\n        batch = tf.shape(z_mean_s)[0]\n        dim = tf.shape(z_mean_s)[1]\n        epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=1.0)\n        return z_mean_s + tf.exp(0.5 * z_log_var_s) * epsilon\n\n    z_sampled_tensor = Lambda(sampling, output_shape=(LATENT_DIM,), name='z_sampling_lambda')([z_mean_tensor, z_log_var_tensor])\n\n    # VAE Decoder\n    decoder_h_layer1 = Dense(int(original_dim * 1.0), activation='relu', name='decoder_h1')\n    decoder_h_layer2 = Dense(int(original_dim * 1.5), activation='relu', name='decoder_h2')\n    decoder_mean_layer = Dense(original_dim, activation=None, name='reconstruction_output_layer') # 命名輸出層\n\n    h_decoded_vae = decoder_h_layer1(z_sampled_tensor)\n    h_decoded_vae = decoder_h_layer2(h_decoded_vae)\n    x_reconstructed_tensor_out = decoder_mean_layer(h_decoded_vae)\n\n    # VAE 模型現在只輸出重構的 x\n    vae = Model(inputs_vae, x_reconstructed_tensor_out, name='vae_with_internal_kl_loss')\n\n    # 損失函數現在只需要計算重構損失\n    def reconstruction_loss_fn(y_true, y_pred):\n        return tf.reduce_mean(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1))\n\n    # 編譯模型\n    # KL 散度是通過 KLDivergenceLayer 內部調用 self.add_loss() 添加的\n    # Keras 在訓練時會自動收集所有通過 add_loss（層級別）添加的損失，\n    # 並將它們與 compile 中指定的損失相加。\n    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                loss=reconstruction_loss_fn)\n    vae.summary()\n\n    print(\"\\n訓練 VAE...\")\n    # KL 散度作為一個額外的損失項被加到總損失中\n    history = vae.fit(X_train_minority, X_train_minority,\n                    epochs=VAE_EPOCHS,\n                    batch_size=VAE_BATCH_SIZE,\n                    shuffle=True,\n                    verbose=1)\n\n    if history and history.history and 'loss' in history.history:\n        plt.figure(figsize=(8, 4))\n        plt.plot(history.history['loss'])\n        plt.title('VAE Model Total Loss (Reconstruction + KL)')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.show()\n\n    # 創建獨立的 Decoder/Generator 模型\n    generator_input_tensor = Input(shape=(LATENT_DIM,), name='standalone_generator_input')\n    _h_decoded_gen = decoder_h_layer1(generator_input_tensor)\n    _h_decoded_gen = decoder_h_layer2(_h_decoded_gen)\n    _x_decoded_mean_gen = decoder_mean_layer(_h_decoded_gen) # 使用相同的輸出層實例\n    generator = Model(generator_input_tensor, _x_decoded_mean_gen, name='standalone_generator')\n\n    # 匯出模型\n    print(f\"\\n正在匯出 VAE 模型到 {vae_model_path}...\")\n    vae.save(vae_model_path)\n    print(\"VAE 模型已匯出。\")\n\n    print(f\"\\n正在匯出 Generator 模型到 {generator_model_path}...\")\n    generator.save(generator_model_path)\n    print(\"Generator 模型已匯出。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.653580Z","iopub.execute_input":"2025-06-08T15:34:46.654044Z","iopub.status.idle":"2025-06-08T15:34:46.905061Z","shell.execute_reply.started":"2025-06-08T15:34:46.654022Z","shell.execute_reply":"2025-06-08T15:34:46.904457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n================ VAE 生成數據 ================\")\n# 根據TARGET_RATIO計算是否需要生成樣本+要生成多少樣本\nif hasattr(y_train, 'values'):\n    y_values = y_train.values.ravel() # ravel()確保它是一維的\nelse:\n    y_values = np.asarray(y_train).ravel()\n\n# 使用np.sum()，它通常會返回一個純量（Python int或NumPy int）\nnum_majority = np.sum(y_values == 0)\nnum_minority_original = np.sum(y_values == 1)\nprint(f'num_majority: {num_majority}, num_minority_original: {num_minority_original}')\n\ntarget_num_minority = int(round(num_majority / TARGET_RATIO))\nprint(f'target_num_minority: {target_num_minority}')\n\nnum_to_generate = target_num_minority - num_minority_original\nnum_to_generate = max(0, num_to_generate)\nprint(f'num_to_generate: {num_to_generate}')\n\nif len(X_train_minority) == 0:\n    print(\"錯誤：訓練集中沒有少數類別樣本，無法訓練 VAE。\")\nelif TARGET_RATIO == 60:\n    print(\"提示：維持訓練集原有1:60比例，將不訓練VAE模型。\")\nelse:\n    if num_to_generate > 0:\n        print(f\"需要生成 {num_to_generate} 個新的少數類別樣本以平衡訓練集。\")\n        random_latent_vectors = np.random.normal(size=(num_to_generate, LATENT_DIM))\n        generated_samples = generator.predict(random_latent_vectors)\n\n        X_train_vae_augmented = np.vstack([X_train, generated_samples])\n        y_train_numpy = y_train.values if isinstance(y_train, pd.Series) else y_train.values\n        \n        # 檢查 y_train_numpy 是否為 2D 且只有一列，如果是，則展平 (flatten) 它\n        if y_train_numpy.ndim == 2 and y_train_numpy.shape[1] == 1:\n            y_train_numpy = y_train_numpy.flatten() # 或者 y_train_numpy.ravel()\n        \n        # 現在 y_train_numpy 應該是 1D 的\n        # np.ones(num_to_generate, dtype=int) 本來就是 1D 的\n        y_train_vae_augmented = np.hstack([y_train_numpy, np.ones(num_to_generate, dtype=int)])\n        \n        shuffle_indices = np.random.permutation(len(X_train_vae_augmented))\n        \"\"\"\n        替換X_train與y_train為資料擴增後的版本\n        \"\"\"\n        X_train_augmented_shuffled_np = X_train_vae_augmented[shuffle_indices]\n        y_train_augmented_shuffled_np = y_train_vae_augmented[shuffle_indices]\n\n        # 將NumPy轉換回Pandas DF\n        X_train = pd.DataFrame(X_train_augmented_shuffled_np,\n                                        columns=X_train.columns)\n        \n        y_train = pd.DataFrame(y_train_augmented_shuffled_np,\n                                         columns=y_train.columns)\n        \n        print(f\"VAE 增強後訓練集大小: {X_train_vae_augmented.shape}\")\n        print(f\"VAE 增強後訓練集目標分佈:\\n{pd.Series(y_train_vae_augmented).value_counts(normalize=True)}\")\n    else:\n        print(\"訓練集已平衡或少數類別更多，無需使用 VAE 生成樣本。\")\n        print(\"VAE 模型未用於增強數據，將跳過 VAE 模型的獨立評估。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.906278Z","iopub.execute_input":"2025-06-08T15:34:46.906467Z","iopub.status.idle":"2025-06-08T15:34:48.287518Z","shell.execute_reply.started":"2025-06-08T15:34:46.906452Z","shell.execute_reply":"2025-06-08T15:34:48.286860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = lgb.LGBMClassifier(\n    # scale_pos_weight=TARGET_RATIO, # 目前情境不用此參數F1反而不錯...\n    learning_rate=0.1,\n    max_depth=20,\n    min_child_samples=300, \n    n_estimators=550, \n    lambda_l1=0.7,\n    lambda_l2=0.3, \n    feature_fraction=1,\n    bagging_fraction=1,\n    bagging_freq=1,\n    random_state=42,\n    verbose=-1,\n    colsample_bytree=0.9,\n    subsample=0.86,\n    n_jobs=-1\n)\n\n# 訓練新模型（啟用提前停止，記錄 Logloss 曲線）\nfinal_model.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_metric=['binary_logloss', 'auc'],\n    eval_names=['train', 'test'],\n    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]  # 提前停止：50 輪內無提升則停止\n)\n\n# 特徵重要性\nfeature_names = X_train.columns.tolist()\nimportance = final_model.feature_importances_\nfeature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\nfeature_df = feature_df.sort_values(by='Importance', ascending=False)\nnew_feature_importance_file = 'feature_importance.csv' if INIT_MODEL else 'feature_importance_final_model_report.csv'\nfeature_df.to_csv(new_feature_importance_file, index=False)\nprint(\"特徵重要性已儲存\")\n\n# 繪製 Logloss 曲線\nplt.figure(figsize=(10, 6))\nplt.plot(final_model.evals_result_['train']['binary_logloss'], label='Train Logloss', color='blue')\nplt.plot(final_model.evals_result_['test']['binary_logloss'], label='Test Logloss', color='orange')\nplt.xlabel('Boosting Rounds')\nplt.ylabel('Logloss')\nplt.title('Training and Test Logloss Curve')\nplt.legend()\nplt.grid(True)\nplt.savefig('logloss_curve.png')\nplt.show()\nprint(\"Logloss 曲線已繪製並儲存為 'logloss_curve.png'\")","metadata":{"_cell_guid":"545a91b0-e73c-4d74-a0a2-5562270a5f39","_uuid":"e6e24f3d-5137-4913-bc55-21947e050f4d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:48.288255Z","iopub.execute_input":"2025-06-08T15:34:48.288476Z","iopub.status.idle":"2025-06-08T15:34:50.962229Z","shell.execute_reply.started":"2025-06-08T15:34:48.288460Z","shell.execute_reply":"2025-06-08T15:34:50.961439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 預測機率\nfrom sklearn.metrics import auc, confusion_matrix, precision_recall_curve, roc_curve\n\n\nprint(\"開始預測...\")\nmodel = final_model\ny_pred_proba = model.predict_proba(X_test)[:, 1]  # 正類機率 (二分類)\n\n# 閾值優化\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\nf1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)  # 計算F1分數\noptimal_idx = np.argmax(f1_scores)  # 找到最佳F1分數的索引\noptimal_threshold = thresholds[optimal_idx]  # 對應的最佳閾值\ny_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)  # 使用最佳閾值預測\n\n# 顯示結果\nbest_f1 = f1_score(y_test, y_pred_optimal)\nprint(f\"最佳閾值: {optimal_threshold:.4f}\")\nprint(f\"最佳 F1 分數: {best_f1:.4f}\")\nprint(\"\\n最佳分類報告:\")\nprint(classification_report(y_test, y_pred_optimal))\nprint(\"最佳閾值下混淆矩陣:\\n\", confusion_matrix(y_test, y_pred_optimal))\n\n# 儲存最佳閾值\nwith open('best_threshold.txt', 'w') as f:\n    f.write(str(optimal_threshold))\nprint(f\"最佳閾值已儲存至 'best_threshold.txt'\")\n\n# 繪製 ROC 曲線（作為最終評估）\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')\nplt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.grid(True)\nplt.savefig('roc_curve.png')\nplt.show()\nprint(\"ROC 曲線已繪製並儲存為 roc_curve.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:50.962964Z","iopub.execute_input":"2025-06-08T15:34:50.963182Z","iopub.status.idle":"2025-06-08T15:34:51.357604Z","shell.execute_reply.started":"2025-06-08T15:34:50.963165Z","shell.execute_reply":"2025-06-08T15:34:51.356844Z"}},"outputs":[],"execution_count":null}]}