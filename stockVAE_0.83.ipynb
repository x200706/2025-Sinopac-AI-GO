{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12097625,"sourceType":"datasetVersion","datasetId":7615931},{"sourceId":428757,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":349494,"modelId":370755}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"MEMOï¼šKaggleä»£ç¢¼ç‚ºæœ€çµ‚ç‰ˆæœ¬","metadata":{}},{"cell_type":"code","source":"\"\"\"\nåœ¨åœ°ç«¯åŸ·è¡Œæ™‚è«‹è¨»è§£æ­¤æ®µç¨‹å¼\n\"\"\"\n!pip install scikit-learn==1.2.2\n!pip install imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:02.149687Z","iopub.execute_input":"2025-06-08T15:34:02.149981Z","iopub.status.idle":"2025-06-08T15:34:08.285412Z","shell.execute_reply.started":"2025-06-08T15:34:02.149962Z","shell.execute_reply":"2025-06-08T15:34:08.284605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# å°å…¥åº«\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\nimport dask\nimport joblib\nimport gc\nimport os\nimport lightgbm as lgb\nfrom sklearn.metrics import f1_score, classification_report, accuracy_score, roc_auc_score, average_precision_score, confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\nfrom sklearn.metrics import make_scorer, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_cell_guid":"79ba8737-9354-4526-8388-c9a3167fb858","_uuid":"68ce0588-31f4-42ac-bd8a-fbebcee925d7","collapsed":false,"execution":{"iopub.status.busy":"2025-06-08T15:34:08.287014Z","iopub.execute_input":"2025-06-08T15:34:08.287267Z","iopub.status.idle":"2025-06-08T15:34:08.293230Z","shell.execute_reply.started":"2025-06-08T15:34:08.287244Z","shell.execute_reply":"2025-06-08T15:34:08.292451Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# åƒæ•¸è¨­ç½®\nRUN_ON_KAGGLE = True\nEXPORT_DATASET_FOR_KAGGEL = False\n\n# æ­£:è² æ¯”ä¾‹å¯¦é©—\n# TARGET_RATIO = 60 # 1:60ï¼Œå³ä¸ä½¿ç”¨VAEæ™‚ F1ç‚º0.8333\nTARGET_RATIO = 50 # 1:50æ™‚ F1ç‚º0.8379 ğŸ‘‘å¿«åˆæœ‰æ•ˆçš„æ¯”ä¾‹\n# TARGET_RATIO = 25 # 1:25æ™‚ F1ç‚º0.8379\n# TARGET_RATIO = 1 # 1:1æ™‚ F1ç‚º0.8379\n\nINIT_MODEL = False\n\n# for VAE\nLATENT_DIM = 12000 # VAE æ½›åœ¨ç©ºé–“ç¶­åº¦ï¼›12000æ˜¯åŸæœ¬çš„ç‰¹å¾µæ¬„ä½å¤§ç•¥æ•¸ç›®ï¼Œå¾ŒçºŒæœƒåˆ·æ–°ç‚º ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸XXX çš„æ¬„ä½æ•¸ç›®\nVAE_EPOCHS = 150 # VAE è¨“ç·´è¼ªæ•¸\nVAE_BATCH_SIZE = 16 # VAE æ‰¹æ¬¡å¤§å°\nRE_TRAIN_VAE = False # VAE_EPOCHSè·ŸVAE_BATCH_SIZEæœ‰æ”¹è®Šæ™‚ï¼Œè¦æ”¹è®Šé€™å€‹è¨­å®šä»¥é‡æ–°è¨“ç·´æ–°çš„VAEæ¨¡å‹","metadata":{"_cell_guid":"f26ec7cd-f75f-4575-a538-d891726de703","_uuid":"f6278279-18ee-4588-9ae0-8b76b51e5995","collapsed":false,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.495449Z","iopub.execute_input":"2025-06-08T15:34:38.496007Z","iopub.status.idle":"2025-06-08T15:34:38.500024Z","shell.execute_reply.started":"2025-06-08T15:34:38.495983Z","shell.execute_reply":"2025-06-08T15:34:38.499214Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nè‡ªå®¶é›»è…¦åƒ…ä½œè³‡æ–™å‰è™•ç†ï¼Œä¸åšVAEæ¨¡å‹è¨“ç·´ï¼Œæ•…ä¸import tfç›¸é—œå¥—ä»¶\n\"\"\"\nif RUN_ON_KAGGLE:\n    # for VAE\n    # pip install tensorflow\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda, Layer, Reshape, Flatten, ReLU, LeakyReLU\n    from tensorflow.keras.layers import BatchNormalization\n    from tensorflow.keras.models import Model, Sequential\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.losses import mse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.502402Z","iopub.execute_input":"2025-06-08T15:34:38.502611Z","iopub.status.idle":"2025-06-08T15:34:38.517084Z","shell.execute_reply.started":"2025-06-08T15:34:38.502588Z","shell.execute_reply":"2025-06-08T15:34:38.516391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not RUN_ON_KAGGLE:\n    \"\"\"\n    ä½¿ç”¨ç¼ºå¤±å€¼å¡«è£œ+æ¬ æ¡æ¨£å®Œçš„è³‡æ–™é›†é€²è¡Œé è™•ç†\n    \"\"\"\n    # è®€å– CSV\n    print('è¼‰å…¥è¨“ç·´é›†ä»¥é€²è¡Œé è™•ç†...')\n    X_train_ddf = dd.read_csv('./X_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n    y_train_ddf = dd.read_csv('./y_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    print('è¼‰å…¥æ¸¬è©¦é›†ä»¥é€²è¡Œé è™•ç†...')\n    X_test_ddf = dd.read_csv('./X_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n    y_test_ddf = dd.read_csv('./y_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.518135Z","iopub.execute_input":"2025-06-08T15:34:38.518385Z","iopub.status.idle":"2025-06-08T15:34:38.530474Z","shell.execute_reply.started":"2025-06-08T15:34:38.518363Z","shell.execute_reply":"2025-06-08T15:34:38.529911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not RUN_ON_KAGGLE:\n    print('ç²¾ç…‰ç‰¹å¾µ...')\n    feature_importance_df = pd.read_csv('feature_importance.csv')\n    # æ”¹ç”¨ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸XXXï¼Œç¯€çœæ•ˆèƒ½+ä¿åº•F1 0.81\n    mask = feature_importance_df['Feature'].str.startswith('ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸', na=False)\n    important_features = feature_importance_df[mask]['Feature'].tolist()\n    print(f\"ä¿ç•™çš„ç‰¹å¾µæ•¸é‡ (åŸºæ–¼å‰ç¶´ 'ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸'): {len(important_features)}\")\n    print(f\"ä¿ç•™çš„ç‰¹å¾µ (åŸºæ–¼å‰ç¶´ 'ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸'):\", important_features)\n\n    # åˆ·æ–°VAEæ½›åœ¨ç©ºé–“ç¶­åº¦åƒæ•¸\n    LATENT_DIM = len(important_features)\n\n    X_train_ddf = X_train_ddf[important_features]\n    X_test_ddf = X_test_ddf[important_features]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.531519Z","iopub.execute_input":"2025-06-08T15:34:38.531787Z","iopub.status.idle":"2025-06-08T15:34:38.545297Z","shell.execute_reply.started":"2025-06-08T15:34:38.531768Z","shell.execute_reply":"2025-06-08T15:34:38.544516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if (not RUN_ON_KAGGLE) and EXPORT_DATASET_FOR_KAGGEL :\n    # åŒ¯å‡ºåˆ°CSV\n    print(\"åŒ¯å‡ºCSVä»¥æ¸…ç©ºè¨ˆç®—åœ–\")\n    X_test_ddf.to_csv('./X_test_ddf_kaggle.csv', index=False)\n    # y_train_ddf.to_csv('./y_train_ddf_kaggle.csv', index=False) # Yæœ¬ä¾†å°±åªæœ‰ä¸€æ¬„ï¼Œä¸ç”¨èŠ±æ™‚é–“é‡æ–°åŒ¯å‡º\n    X_train_ddf.to_csv('./X_train_ddf_kaggle.csv', index=False)\n    # y_test_ddf.to_csv('./y_test_ddf_kaggle.csv', index=False) # Yæœ¬ä¾†å°±åªæœ‰ä¸€æ¬„ï¼Œä¸ç”¨èŠ±æ™‚é–“é‡æ–°åŒ¯å‡º\n    print(\"æ‰€æœ‰è³‡æ–™å·²åŒ¯å‡ºåˆ° CSVï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.546481Z","iopub.execute_input":"2025-06-08T15:34:38.546747Z","iopub.status.idle":"2025-06-08T15:34:38.562891Z","shell.execute_reply.started":"2025-06-08T15:34:38.546727Z","shell.execute_reply":"2025-06-08T15:34:38.562213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nè‡ªå®¶é›»è…¦åƒ…ä½œè³‡æ–™å‰è™•ç†ï¼Œä¸åšVAEæ¨¡å‹è¨“ç·´ï¼Œæ•…ä¸éœ€è¦Pandas df\n\"\"\"\n# if not RUN_ON_KAGGLE:\n#     \"\"\"\n#     æ­¤æ™‚æ‰æ­£å¼å°‡ddfæ­£å¼è½‰æˆPandas df\n#     \"\"\"\n#     print(\"è½‰æ›è¨“ç·´é›†æ ¼å¼ç‚ºç†Šè²“DF...\")\n#     # åˆå§‹åŒ–ç©ºçš„ DataFrame\n#     X_train = pd.DataFrame()\n#     y_train = pd.DataFrame()\n\n#     # åˆ†æ‰¹åˆä½µ\n#     batch_size = 10  # æ¯æ¬¡è™•ç† 10 å€‹åˆ†å‰²\n#     for i in range(0, X_train_ddf.npartitions, batch_size):\n#         print(f\"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}\")\n#         # ä¸€æ¬¡è¨ˆç®—å¤šå€‹åˆ†å‰²\n#         X_batch = X_train_ddf.partitions[i:min(i + batch_size, X_train_ddf.npartitions)].compute()\n#         y_batch = y_train_ddf.partitions[i:min(i + batch_size, y_train_ddf.npartitions)].compute()\n#         # åˆä½µåˆ°çµæœ\n#         X_train = pd.concat([X_train, X_batch], axis=0, ignore_index=True)\n#         y_train = pd.concat([y_train, y_batch], axis=0, ignore_index=True)\n#         del X_batch, y_batch\n#         gc.collect()\n\n#     print(\"è½‰æ›æ¸¬è©¦é›†æ ¼å¼ç‚ºç†Šè²“DF...\")\n#     # åˆå§‹åŒ–ç©ºçš„ DataFrame\n#     X_test = pd.DataFrame()\n#     y_test = pd.DataFrame()\n\n#     # åˆ†æ‰¹åˆä½µ\n#     batch_size = 10  # æ¯æ¬¡è™•ç† 10 å€‹åˆ†å‰²\n#     for i in range(0, X_test_ddf.npartitions, batch_size):\n#         print(f\"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}\")\n#         # ä¸€æ¬¡è¨ˆç®—å¤šå€‹åˆ†å‰²\n#         X_batch = X_test_ddf.partitions[i:min(i + batch_size, X_test_ddf.npartitions)].compute()\n#         y_batch = y_test_ddf.partitions[i:min(i + batch_size, y_test_ddf.npartitions)].compute()\n#         # åˆä½µåˆ°çµæœ\n#         X_test = pd.concat([X_test, X_batch], axis=0, ignore_index=True)\n#         y_test = pd.concat([y_test, y_batch], axis=0, ignore_index=True)\n#         del X_batch, y_batch\n#         gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.563517Z","iopub.execute_input":"2025-06-08T15:34:38.563729Z","iopub.status.idle":"2025-06-08T15:34:38.577052Z","shell.execute_reply.started":"2025-06-08T15:34:38.563692Z","shell.execute_reply":"2025-06-08T15:34:38.576519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nè‡ªå®¶é›»è…¦åƒ…ä½œè³‡æ–™å‰è™•ç†ï¼Œä¸åšVAEæ¨¡å‹è¨“ç·´ï¼Œæå‰çµæŸç¨‹å¼\n\"\"\"\nimport sys\nif not RUN_ON_KAGGLE:\n    sys.exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.578387Z","iopub.execute_input":"2025-06-08T15:34:38.578564Z","iopub.status.idle":"2025-06-08T15:34:38.593543Z","shell.execute_reply.started":"2025-06-08T15:34:38.578551Z","shell.execute_reply":"2025-06-08T15:34:38.592975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_ON_KAGGLE:\n    \"\"\"\n    ä½¿ç”¨ç¼ºå¤±å€¼å¡«è£œ+æ¬ æ¡æ¨£å®Œ+åƒ…ç•™ä¸Šå¸‚åŠ æ¬ŠæŒ‡æ•¸XXXçš„è³‡æ–™é›†\n    \"\"\"\n    # è®€å– CSV\n    print(\"è½‰æ›è¨“ç·´é›†æ ¼å¼ç‚ºç†Šè²“DF...\")\n    X_train_ddf = dd.read_csv('/kaggle/input/taiwan-stock/X_train_ddf_kaggle.csv/*', sample=1048576, blocksize='128MB') #TODO æ‡‰è©²æ”¹æˆKaggleå¯¦éš›ç›®éŒ„\n    y_train_ddf = dd.read_csv('/kaggle/input/taiwan-stock/y_train_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    # åˆå§‹åŒ–ç©ºçš„ DataFrame\n    X_train = pd.DataFrame()\n    y_train = pd.DataFrame()\n\n    # åˆ†æ‰¹åˆä½µ\n    batch_size = 10  # æ¯æ¬¡è™•ç† 10 å€‹åˆ†å‰²\n    for i in range(0, X_train_ddf.npartitions, batch_size):\n        print(f\"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}\")\n        # ä¸€æ¬¡è¨ˆç®—å¤šå€‹åˆ†å‰²\n        X_batch = X_train_ddf.partitions[i:min(i + batch_size, X_train_ddf.npartitions)].compute()\n        y_batch = y_train_ddf.partitions[i:min(i + batch_size, y_train_ddf.npartitions)].compute()\n        # åˆä½µåˆ°çµæœ\n        X_train = pd.concat([X_train, X_batch], axis=0, ignore_index=True)\n        y_train = pd.concat([y_train, y_batch], axis=0, ignore_index=True)\n        del X_batch, y_batch\n        gc.collect()\n\n    # è®€å– CSV\n    print(\"è½‰æ›æ¸¬è©¦é›†æ ¼å¼ç‚ºç†Šè²“DF...\")\n    X_test_ddf = dd.read_csv('/kaggle/input/taiwan-stock/X_test_ddf_kaggle.csv/*', sample=1048576, blocksize='128MB')\n    y_test_ddf = dd.read_csv('/kaggle/input/taiwan-stock/y_test_ddf.csv/*', sample=1048576, blocksize='128MB')\n\n    # åˆå§‹åŒ–ç©ºçš„ DataFrame\n    X_test = pd.DataFrame()\n    y_test = pd.DataFrame()\n\n    # åˆ†æ‰¹åˆä½µ\n    batch_size = 10  # æ¯æ¬¡è™•ç† 10 å€‹åˆ†å‰²\n    for i in range(0, X_test_ddf.npartitions, batch_size):\n        print(f\"è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}\")\n        # ä¸€æ¬¡è¨ˆç®—å¤šå€‹åˆ†å‰²\n        X_batch = X_test_ddf.partitions[i:min(i + batch_size, X_test_ddf.npartitions)].compute()\n        y_batch = y_test_ddf.partitions[i:min(i + batch_size, y_test_ddf.npartitions)].compute()\n        # åˆä½µåˆ°çµæœ\n        X_test = pd.concat([X_test, X_batch], axis=0, ignore_index=True)\n        y_test = pd.concat([y_test, y_batch], axis=0, ignore_index=True)\n        del X_batch, y_batch\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:38.614659Z","iopub.execute_input":"2025-06-08T15:34:38.615236Z","iopub.status.idle":"2025-06-08T15:34:46.629000Z","shell.execute_reply.started":"2025-06-08T15:34:38.615220Z","shell.execute_reply":"2025-06-08T15:34:46.628168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# è¨ˆç®—æ­£è² æ¨£æœ¬æ¬Šé‡\nneg_count_result = (y_train == 0).sum() # è¦ç•™æ„Pandas dfè·ŸDask ddè¨ˆç®—è¡Œç‚ºæœ‰äº›è¨±ä¸åŒ\npos_count_result = (y_train == 1).sum()\n\n# æ‰“å°é¡å‹ä»¥èª¿è©¦\nprint(f\"Type of neg_count_result: {type(neg_count_result)}\")\nprint(f\"Value of neg_count_result: {neg_count_result}\")\nprint(f\"Type of pos_count_result: {type(pos_count_result)}\")\nprint(f\"Value of pos_count_result: {pos_count_result}\")\n\n# å¾å¯èƒ½çš„ Series ä¸­æå–ç´”é‡\nneg_count = neg_count_result.item() if isinstance(neg_count_result, pd.Series) else neg_count_result\npos_count = pos_count_result.item() if isinstance(pos_count_result, pd.Series) else pos_count_result\n\nscale_pos_weight = neg_count / pos_count if pos_count > 0 else 1\n\nprint(f\"Negative count: {neg_count}\")\nprint(f\"Positive count: {pos_count}\")\nprint(f\"Scale positive weight: {scale_pos_weight}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.630300Z","iopub.execute_input":"2025-06-08T15:34:46.630533Z","iopub.status.idle":"2025-06-08T15:34:46.638422Z","shell.execute_reply.started":"2025-06-08T15:34:46.630516Z","shell.execute_reply":"2025-06-08T15:34:46.637851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\næ­¤å¡ŠforåŒ¯å…¥.kerasæ¨¡å‹\n\"\"\"\n# KL Divergence Loss Layer\nclass KLDivergenceLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(KLDivergenceLayer, self).__init__(**kwargs)\n\n    def call(self, inputs_for_kl): # inputs_for_kl æ‡‰è©²æ˜¯ [z_mean, z_log_var]\n        z_mean_kl, z_log_var_kl = inputs_for_kl\n        kl_batch = 1 + z_log_var_kl - tf.square(z_mean_kl) - tf.exp(z_log_var_kl)\n        kl_batch = tf.reduce_sum(kl_batch, axis=-1) * -0.5\n        self.add_loss(tf.reduce_mean(kl_batch)) # æ·»åŠ  KL æå¤±çš„å‡å€¼åˆ°å±¤çš„æå¤±\n        return inputs_for_kl # è¿”å›åŸå§‹è¼¸å…¥ï¼Œä¸æ”¹è®Šæ•¸æ“šæµ\n\n# Sampling Layer (è¼¸å…¥ä»ç„¶æ˜¯åŸå§‹çš„ z_mean å’Œ z_log_var)\ndef sampling(args):\n    z_mean_s, z_log_var_s = args\n    batch = tf.shape(z_mean_s)[0]\n    dim = tf.shape(z_mean_s)[1]\n    epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=1.0)\n    return z_mean_s + tf.exp(0.5 * z_log_var_s) * epsilon\n\n# æå¤±å‡½æ•¸ç¾åœ¨åªéœ€è¦è¨ˆç®—é‡æ§‹æå¤±\ndef reconstruction_loss_fn(y_true, y_pred):\n    return tf.reduce_mean(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.639299Z","iopub.execute_input":"2025-06-08T15:34:46.639557Z","iopub.status.idle":"2025-06-08T15:34:46.652852Z","shell.execute_reply.started":"2025-06-08T15:34:46.639541Z","shell.execute_reply":"2025-06-08T15:34:46.652216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nVAEéæ¡æ¨£è‡³æŒ‡å®šæ¯”ä¾‹\n\"\"\"\nprint(\"\\n================ VAE æ¨¡å‹è¨“ç·´ ================\")\n#TODO è¦æ”¹æˆKaggleè·¯å¾‘ æ³¨æ„è¦ä¸Šå‚³åˆ°input\nvae_model_path = 'my_vae.keras'\ngenerator_model_path = 'my_vae_generator.keras'\nX_train_minority = X_train[y_train.values == 1] # é€™è£¡æ±ºå®šç”Ÿæˆç›®æ¨™\n\nif os.path.exists('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae.keras') and os.path.exists('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae_generator.keras') and RE_TRAIN_VAE != True: # å¦‚æœç›®éŒ„ä¸‹å·²ç¶“å­˜åœ¨VAEæ¨¡å‹ï¼Œä»£è¡¨å·²è¨“ç·´å®Œæˆï¼Œç„¡é ˆé‡æ–°è¨“ç·´\n    print(\"\\næ­£åœ¨è¼‰å…¥æ¨¡å‹...\")\n    custom_objects_dict = {\n        'sampling': sampling,\n        'reconstruction_loss_fn': reconstruction_loss_fn\n    }\n    vae = tf.keras.models.load_model('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae.keras', custom_objects=custom_objects_dict)\n    print(\"VAE æ¨¡å‹å·²è¼‰å…¥ã€‚\")\n    vae.summary()\n\n    generator = tf.keras.models.load_model('/kaggle/input/stock-vae/tensorflow2/default/1/my_vae_generator.keras')\n    print(\"Generator æ¨¡å‹å·²è¼‰å…¥ã€‚\")\n    generator.summary()\nelse:\n    print(f\"ç”¨æ–¼è¨“ç·´ VAE çš„å°‘æ•¸é¡åˆ¥æ¨£æœ¬æ•¸é‡: {len(X_train_minority)}\")\n    original_dim = X_train_minority.shape[1]\n\n    # VAE Encoder\n    inputs_vae = Input(shape=(original_dim,), name='vae_input')\n    h_vae = Dense(int(original_dim * 2.0), activation='relu')(inputs_vae)\n    h_vae = BatchNormalization()(h_vae)\n    h_vae = Dense(int(original_dim * 1.25), activation='relu')(h_vae)\n    z_mean_tensor = Dense(LATENT_DIM, name='z_mean_encoder_output')(h_vae)\n    z_log_var_tensor = Dense(LATENT_DIM, name='z_log_var_encoder_output')(h_vae)\n\n    # KL Divergence Loss Layer\n    class KLDivergenceLayer(tf.keras.layers.Layer):\n        def __init__(self, **kwargs):\n            super(KLDivergenceLayer, self).__init__(**kwargs)\n\n        def call(self, inputs_for_kl): # inputs_for_kl æ‡‰è©²æ˜¯ [z_mean, z_log_var]\n            z_mean_kl, z_log_var_kl = inputs_for_kl\n            kl_batch = 1 + z_log_var_kl - tf.square(z_mean_kl) - tf.exp(z_log_var_kl)\n            kl_batch = tf.reduce_sum(kl_batch, axis=-1) * -0.5\n            self.add_loss(tf.reduce_mean(kl_batch)) # æ·»åŠ  KL æå¤±çš„å‡å€¼åˆ°å±¤çš„æå¤±\n            return inputs_for_kl # è¿”å›åŸå§‹è¼¸å…¥ï¼Œä¸æ”¹è®Šæ•¸æ“šæµ\n\n    # å°‡ KL æ•£åº¦è¨ˆç®—å±¤æ‡‰ç”¨æ–¼ encoder çš„è¼¸å‡º\n    # é€™å€‹å±¤çš„ call æ–¹æ³•æœƒåŸ·è¡Œ self.add_loss()\n    # _ (ä¸‹åŠƒç·š) è¡¨ç¤ºæˆ‘å€‘ä¸ç›´æ¥ä½¿ç”¨é€™å€‹å±¤çš„è¼¸å‡ºé€²è¡Œå¾ŒçºŒè¨ˆç®—ï¼ˆå› ç‚ºå®ƒè¿”å›è¼¸å…¥ï¼‰\n    # ä½†é‡è¦çš„æ˜¯å®ƒè¢«åŸ·è¡Œäº†ã€‚\n    _ = KLDivergenceLayer(name='kl_divergence_adder')([z_mean_tensor, z_log_var_tensor])\n\n\n    # Sampling Layer (è¼¸å…¥ä»ç„¶æ˜¯åŸå§‹çš„ z_mean å’Œ z_log_var)\n    def sampling(args):\n        z_mean_s, z_log_var_s = args\n        batch = tf.shape(z_mean_s)[0]\n        dim = tf.shape(z_mean_s)[1]\n        epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=1.0)\n        return z_mean_s + tf.exp(0.5 * z_log_var_s) * epsilon\n\n    z_sampled_tensor = Lambda(sampling, output_shape=(LATENT_DIM,), name='z_sampling_lambda')([z_mean_tensor, z_log_var_tensor])\n\n    # VAE Decoder\n    decoder_h_layer1 = Dense(int(original_dim * 1.0), activation='relu', name='decoder_h1')\n    decoder_h_layer2 = Dense(int(original_dim * 1.5), activation='relu', name='decoder_h2')\n    decoder_mean_layer = Dense(original_dim, activation=None, name='reconstruction_output_layer') # å‘½åè¼¸å‡ºå±¤\n\n    h_decoded_vae = decoder_h_layer1(z_sampled_tensor)\n    h_decoded_vae = decoder_h_layer2(h_decoded_vae)\n    x_reconstructed_tensor_out = decoder_mean_layer(h_decoded_vae)\n\n    # VAE æ¨¡å‹ç¾åœ¨åªè¼¸å‡ºé‡æ§‹çš„ x\n    vae = Model(inputs_vae, x_reconstructed_tensor_out, name='vae_with_internal_kl_loss')\n\n    # æå¤±å‡½æ•¸ç¾åœ¨åªéœ€è¦è¨ˆç®—é‡æ§‹æå¤±\n    def reconstruction_loss_fn(y_true, y_pred):\n        return tf.reduce_mean(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1))\n\n    # ç·¨è­¯æ¨¡å‹\n    # KL æ•£åº¦æ˜¯é€šé KLDivergenceLayer å…§éƒ¨èª¿ç”¨ self.add_loss() æ·»åŠ çš„\n    # Keras åœ¨è¨“ç·´æ™‚æœƒè‡ªå‹•æ”¶é›†æ‰€æœ‰é€šé add_lossï¼ˆå±¤ç´šåˆ¥ï¼‰æ·»åŠ çš„æå¤±ï¼Œ\n    # ä¸¦å°‡å®ƒå€‘èˆ‡ compile ä¸­æŒ‡å®šçš„æå¤±ç›¸åŠ ã€‚\n    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                loss=reconstruction_loss_fn)\n    vae.summary()\n\n    print(\"\\nè¨“ç·´ VAE...\")\n    # KL æ•£åº¦ä½œç‚ºä¸€å€‹é¡å¤–çš„æå¤±é …è¢«åŠ åˆ°ç¸½æå¤±ä¸­\n    history = vae.fit(X_train_minority, X_train_minority,\n                    epochs=VAE_EPOCHS,\n                    batch_size=VAE_BATCH_SIZE,\n                    shuffle=True,\n                    verbose=1)\n\n    if history and history.history and 'loss' in history.history:\n        plt.figure(figsize=(8, 4))\n        plt.plot(history.history['loss'])\n        plt.title('VAE Model Total Loss (Reconstruction + KL)')\n        plt.ylabel('Loss')\n        plt.xlabel('Epoch')\n        plt.show()\n\n    # å‰µå»ºç¨ç«‹çš„ Decoder/Generator æ¨¡å‹\n    generator_input_tensor = Input(shape=(LATENT_DIM,), name='standalone_generator_input')\n    _h_decoded_gen = decoder_h_layer1(generator_input_tensor)\n    _h_decoded_gen = decoder_h_layer2(_h_decoded_gen)\n    _x_decoded_mean_gen = decoder_mean_layer(_h_decoded_gen) # ä½¿ç”¨ç›¸åŒçš„è¼¸å‡ºå±¤å¯¦ä¾‹\n    generator = Model(generator_input_tensor, _x_decoded_mean_gen, name='standalone_generator')\n\n    # åŒ¯å‡ºæ¨¡å‹\n    print(f\"\\næ­£åœ¨åŒ¯å‡º VAE æ¨¡å‹åˆ° {vae_model_path}...\")\n    vae.save(vae_model_path)\n    print(\"VAE æ¨¡å‹å·²åŒ¯å‡ºã€‚\")\n\n    print(f\"\\næ­£åœ¨åŒ¯å‡º Generator æ¨¡å‹åˆ° {generator_model_path}...\")\n    generator.save(generator_model_path)\n    print(\"Generator æ¨¡å‹å·²åŒ¯å‡ºã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.653580Z","iopub.execute_input":"2025-06-08T15:34:46.654044Z","iopub.status.idle":"2025-06-08T15:34:46.905061Z","shell.execute_reply.started":"2025-06-08T15:34:46.654022Z","shell.execute_reply":"2025-06-08T15:34:46.904457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n================ VAE ç”Ÿæˆæ•¸æ“š ================\")\n# æ ¹æ“šTARGET_RATIOè¨ˆç®—æ˜¯å¦éœ€è¦ç”Ÿæˆæ¨£æœ¬+è¦ç”Ÿæˆå¤šå°‘æ¨£æœ¬\nif hasattr(y_train, 'values'):\n    y_values = y_train.values.ravel() # ravel()ç¢ºä¿å®ƒæ˜¯ä¸€ç¶­çš„\nelse:\n    y_values = np.asarray(y_train).ravel()\n\n# ä½¿ç”¨np.sum()ï¼Œå®ƒé€šå¸¸æœƒè¿”å›ä¸€å€‹ç´”é‡ï¼ˆPython intæˆ–NumPy intï¼‰\nnum_majority = np.sum(y_values == 0)\nnum_minority_original = np.sum(y_values == 1)\nprint(f'num_majority: {num_majority}, num_minority_original: {num_minority_original}')\n\ntarget_num_minority = int(round(num_majority / TARGET_RATIO))\nprint(f'target_num_minority: {target_num_minority}')\n\nnum_to_generate = target_num_minority - num_minority_original\nnum_to_generate = max(0, num_to_generate)\nprint(f'num_to_generate: {num_to_generate}')\n\nif len(X_train_minority) == 0:\n    print(\"éŒ¯èª¤ï¼šè¨“ç·´é›†ä¸­æ²’æœ‰å°‘æ•¸é¡åˆ¥æ¨£æœ¬ï¼Œç„¡æ³•è¨“ç·´ VAEã€‚\")\nelif TARGET_RATIO == 60:\n    print(\"æç¤ºï¼šç¶­æŒè¨“ç·´é›†åŸæœ‰1:60æ¯”ä¾‹ï¼Œå°‡ä¸è¨“ç·´VAEæ¨¡å‹ã€‚\")\nelse:\n    if num_to_generate > 0:\n        print(f\"éœ€è¦ç”Ÿæˆ {num_to_generate} å€‹æ–°çš„å°‘æ•¸é¡åˆ¥æ¨£æœ¬ä»¥å¹³è¡¡è¨“ç·´é›†ã€‚\")\n        random_latent_vectors = np.random.normal(size=(num_to_generate, LATENT_DIM))\n        generated_samples = generator.predict(random_latent_vectors)\n\n        X_train_vae_augmented = np.vstack([X_train, generated_samples])\n        y_train_numpy = y_train.values if isinstance(y_train, pd.Series) else y_train.values\n        \n        # æª¢æŸ¥ y_train_numpy æ˜¯å¦ç‚º 2D ä¸”åªæœ‰ä¸€åˆ—ï¼Œå¦‚æœæ˜¯ï¼Œå‰‡å±•å¹³ (flatten) å®ƒ\n        if y_train_numpy.ndim == 2 and y_train_numpy.shape[1] == 1:\n            y_train_numpy = y_train_numpy.flatten() # æˆ–è€… y_train_numpy.ravel()\n        \n        # ç¾åœ¨ y_train_numpy æ‡‰è©²æ˜¯ 1D çš„\n        # np.ones(num_to_generate, dtype=int) æœ¬ä¾†å°±æ˜¯ 1D çš„\n        y_train_vae_augmented = np.hstack([y_train_numpy, np.ones(num_to_generate, dtype=int)])\n        \n        shuffle_indices = np.random.permutation(len(X_train_vae_augmented))\n        \"\"\"\n        æ›¿æ›X_trainèˆ‡y_trainç‚ºè³‡æ–™æ“´å¢å¾Œçš„ç‰ˆæœ¬\n        \"\"\"\n        X_train_augmented_shuffled_np = X_train_vae_augmented[shuffle_indices]\n        y_train_augmented_shuffled_np = y_train_vae_augmented[shuffle_indices]\n\n        # å°‡NumPyè½‰æ›å›Pandas DF\n        X_train = pd.DataFrame(X_train_augmented_shuffled_np,\n                                        columns=X_train.columns)\n        \n        y_train = pd.DataFrame(y_train_augmented_shuffled_np,\n                                         columns=y_train.columns)\n        \n        print(f\"VAE å¢å¼·å¾Œè¨“ç·´é›†å¤§å°: {X_train_vae_augmented.shape}\")\n        print(f\"VAE å¢å¼·å¾Œè¨“ç·´é›†ç›®æ¨™åˆ†ä½ˆ:\\n{pd.Series(y_train_vae_augmented).value_counts(normalize=True)}\")\n    else:\n        print(\"è¨“ç·´é›†å·²å¹³è¡¡æˆ–å°‘æ•¸é¡åˆ¥æ›´å¤šï¼Œç„¡éœ€ä½¿ç”¨ VAE ç”Ÿæˆæ¨£æœ¬ã€‚\")\n        print(\"VAE æ¨¡å‹æœªç”¨æ–¼å¢å¼·æ•¸æ“šï¼Œå°‡è·³é VAE æ¨¡å‹çš„ç¨ç«‹è©•ä¼°ã€‚\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:46.906278Z","iopub.execute_input":"2025-06-08T15:34:46.906467Z","iopub.status.idle":"2025-06-08T15:34:48.287518Z","shell.execute_reply.started":"2025-06-08T15:34:46.906452Z","shell.execute_reply":"2025-06-08T15:34:48.286860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model = lgb.LGBMClassifier(\n    # scale_pos_weight=TARGET_RATIO, # ç›®å‰æƒ…å¢ƒä¸ç”¨æ­¤åƒæ•¸F1åè€Œä¸éŒ¯...\n    learning_rate=0.1,\n    max_depth=20,\n    min_child_samples=300, \n    n_estimators=550, \n    lambda_l1=0.7,\n    lambda_l2=0.3, \n    feature_fraction=1,\n    bagging_fraction=1,\n    bagging_freq=1,\n    random_state=42,\n    verbose=-1,\n    colsample_bytree=0.9,\n    subsample=0.86,\n    n_jobs=-1\n)\n\n# è¨“ç·´æ–°æ¨¡å‹ï¼ˆå•Ÿç”¨æå‰åœæ­¢ï¼Œè¨˜éŒ„ Logloss æ›²ç·šï¼‰\nfinal_model.fit(\n    X_train,\n    y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_metric=['binary_logloss', 'auc'],\n    eval_names=['train', 'test'],\n    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]  # æå‰åœæ­¢ï¼š50 è¼ªå…§ç„¡æå‡å‰‡åœæ­¢\n)\n\n# ç‰¹å¾µé‡è¦æ€§\nfeature_names = X_train.columns.tolist()\nimportance = final_model.feature_importances_\nfeature_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\nfeature_df = feature_df.sort_values(by='Importance', ascending=False)\nnew_feature_importance_file = 'feature_importance.csv' if INIT_MODEL else 'feature_importance_final_model_report.csv'\nfeature_df.to_csv(new_feature_importance_file, index=False)\nprint(\"ç‰¹å¾µé‡è¦æ€§å·²å„²å­˜\")\n\n# ç¹ªè£½ Logloss æ›²ç·š\nplt.figure(figsize=(10, 6))\nplt.plot(final_model.evals_result_['train']['binary_logloss'], label='Train Logloss', color='blue')\nplt.plot(final_model.evals_result_['test']['binary_logloss'], label='Test Logloss', color='orange')\nplt.xlabel('Boosting Rounds')\nplt.ylabel('Logloss')\nplt.title('Training and Test Logloss Curve')\nplt.legend()\nplt.grid(True)\nplt.savefig('logloss_curve.png')\nplt.show()\nprint(\"Logloss æ›²ç·šå·²ç¹ªè£½ä¸¦å„²å­˜ç‚º 'logloss_curve.png'\")","metadata":{"_cell_guid":"545a91b0-e73c-4d74-a0a2-5562270a5f39","_uuid":"e6e24f3d-5137-4913-bc55-21947e050f4d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:48.288255Z","iopub.execute_input":"2025-06-08T15:34:48.288476Z","iopub.status.idle":"2025-06-08T15:34:50.962229Z","shell.execute_reply.started":"2025-06-08T15:34:48.288460Z","shell.execute_reply":"2025-06-08T15:34:50.961439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# é æ¸¬æ©Ÿç‡\nfrom sklearn.metrics import auc, confusion_matrix, precision_recall_curve, roc_curve\n\n\nprint(\"é–‹å§‹é æ¸¬...\")\nmodel = final_model\ny_pred_proba = model.predict_proba(X_test)[:, 1]  # æ­£é¡æ©Ÿç‡ (äºŒåˆ†é¡)\n\n# é–¾å€¼å„ªåŒ–\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\nf1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)  # è¨ˆç®—F1åˆ†æ•¸\noptimal_idx = np.argmax(f1_scores)  # æ‰¾åˆ°æœ€ä½³F1åˆ†æ•¸çš„ç´¢å¼•\noptimal_threshold = thresholds[optimal_idx]  # å°æ‡‰çš„æœ€ä½³é–¾å€¼\ny_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)  # ä½¿ç”¨æœ€ä½³é–¾å€¼é æ¸¬\n\n# é¡¯ç¤ºçµæœ\nbest_f1 = f1_score(y_test, y_pred_optimal)\nprint(f\"æœ€ä½³é–¾å€¼: {optimal_threshold:.4f}\")\nprint(f\"æœ€ä½³ F1 åˆ†æ•¸: {best_f1:.4f}\")\nprint(\"\\næœ€ä½³åˆ†é¡å ±å‘Š:\")\nprint(classification_report(y_test, y_pred_optimal))\nprint(\"æœ€ä½³é–¾å€¼ä¸‹æ··æ·†çŸ©é™£:\\n\", confusion_matrix(y_test, y_pred_optimal))\n\n# å„²å­˜æœ€ä½³é–¾å€¼\nwith open('best_threshold.txt', 'w') as f:\n    f.write(str(optimal_threshold))\nprint(f\"æœ€ä½³é–¾å€¼å·²å„²å­˜è‡³ 'best_threshold.txt'\")\n\n# ç¹ªè£½ ROC æ›²ç·šï¼ˆä½œç‚ºæœ€çµ‚è©•ä¼°ï¼‰\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='blue')\nplt.plot([0, 1], [0, 1], 'r--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.grid(True)\nplt.savefig('roc_curve.png')\nplt.show()\nprint(\"ROC æ›²ç·šå·²ç¹ªè£½ä¸¦å„²å­˜ç‚º roc_curve.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T15:34:50.962964Z","iopub.execute_input":"2025-06-08T15:34:50.963182Z","iopub.status.idle":"2025-06-08T15:34:51.357604Z","shell.execute_reply.started":"2025-06-08T15:34:50.963165Z","shell.execute_reply":"2025-06-08T15:34:51.356844Z"}},"outputs":[],"execution_count":null}]}